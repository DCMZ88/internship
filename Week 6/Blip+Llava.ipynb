{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c54ff-1d48-4cac-9a6a-e6acee15475b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Llava-hf model for generating questions\n",
    "import requests \n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n",
    "\n",
    "model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n",
    "model= LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    ").to(\"cuda\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5263c46f-473f-47d7-b46b-d0f829da6c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "processor_answer = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model_answer = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3d41566-ea55-4bd1-b896-72b5f952730a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the number of questions\n",
    "questions = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e285ed75-3fe8-4a2b-af62-dfe8a26b36c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For generating the questions\n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          # Input Prompt \n",
    "          {\"type\": \"text\", \"text\": f\"Generate different {questions} questions for the image without any answers that can be easily inferenced from the image \"},\n",
    "          {\"type\": \"image\"}, \n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdd70e-e972-41b6-bada-210cc8bef8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input Image ( Loads the image )\n",
    "image_file = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24894a62-64cb-4bba-9131-4f46d57c1719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Processes the image and text prompt\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=1000, do_sample=False)\n",
    "answer = processor.decode(output[0][2:], skip_special_tokens=True)\n",
    "# Remove the input prompt and only output the answers generated by the VLM\n",
    "lines = answer.split('\\n')\n",
    "lines.pop(1)\n",
    "results = '\\n'.join(lines)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6de1b072-6caf-4048-babe-287c6eba5c48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text has been written to 'output.txt'\n"
     ]
    }
   ],
   "source": [
    "# Saves the questions in a file \n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "    file.write(results)\n",
    "\n",
    "print(\"Text has been written to 'output.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c5d676e-fd3a-4db9-a9e9-a6816eec4048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the file in read mode ('r')\n",
    "with open(\"output.txt\", \"r\") as file:\n",
    "    # Read the contents of the file\n",
    "    questions = file.read()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad2f94b8-1c14-472a-b0f7-cd0481363eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the dog wearing?',\n",
       " 'What is the woman wearing?',\n",
       " 'What is the setting of the image?',\n",
       " 'What is the time of day?',\n",
       " 'What is the weather like?',\n",
       " 'What is the color of the sand?',\n",
       " \"What is the color of the dog's fur?\",\n",
       " \"What is the color of the woman's shirt?\",\n",
       " \"What is the color of the dog's collar?\",\n",
       " \"What is the color of the woman's hair?\",\n",
       " 'What is the color of the waves in the ocean?',\n",
       " 'What is the color of the sky?',\n",
       " 'What is the color of the sand?',\n",
       " \"What is the color of the dog's eyes?\",\n",
       " \"What is the color of the woman's shoes?\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    " \n",
    "# Split the text into each question \n",
    "lined = questions.strip().split(\"\\n\")\n",
    "# The answers from the provided input\n",
    "questions = [line.strip() for line in lined]\n",
    "# Removes the numbers infront of the question\n",
    "cleaned_questions = [re.sub(r'^\\d+\\.\\s*', '', line) for line in questions]\n",
    "cleaned_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b10a8247-13b0-44b4-8c43-48d1c5e7b62c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['harness', 'plaid shirt', 'beach', 'sunset', 'sunny', 'white', 'tan', 'red and white', 'blue', 'brown', 'white', 'white', 'white', 'black', 'black']\n"
     ]
    }
   ],
   "source": [
    "answers = [] \n",
    "for question in cleaned_questions:\n",
    "    inputs = processor_answer(raw_image, question , return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    out = model_answer.generate(**inputs)\n",
    "    output = processor_answer.decode(out[0], skip_special_tokens=True)\n",
    "    answers.append(output)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eba4a7dc-f9cc-428c-9118-e797e44558e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions and answers have been written to 'questions_answers(Blip).json'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Pair the questions with answers\n",
    "qa_data = [{\"question\": question, \"answer\": answer} for question, answer in zip(cleaned_questions, answers)]\n",
    "\n",
    "# Write the generated data to a JSON file\n",
    "with open(\"questions_answers(Blip).json\", \"w\") as file:\n",
    "    json.dump(qa_data, file, indent=4)\n",
    "\n",
    "print(\"Questions and answers have been written to 'questions_answers(Blip).json'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
