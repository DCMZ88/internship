{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef88e6-3bb8-41be-a1e6-7b9c9dd4afc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\"\n",
    ").to(\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2d7a0bb-a63d-43b4-b9a1-75026325772a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. What is the woman doing?\\n2. What is the dog doing?\\n3. What is the setting?\\n4. What is the weather like?\\n5. What is the time of day?\\n6. What is the woman wearing?\\n7. What is the dog wearing?\\n8. What is the dog's leash?\\n9. What is the dog's collar?\\n10. What is the dog's breed?\\n11. What is the dog's name?\\n12. What is the woman's name?\\n13. What is the woman's profession?\\n14. What is the woman's occupation?\\n15. What is the woman's hobby?\\n16. What is the woman's profession?\\n17. What is the woman's occupation?\\n18. What is the woman's hobby?\\n19. What is the woman's profession?\\n20. What is the woman's occupation?\"]\n"
     ]
    }
   ],
   "source": [
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"\"\"Generate 20 simple questions about the image for image inferencing for other visual language models to infer\"\"\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1000)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e63a32fb-48a0-4ffc-88fd-5ff447bc2342",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 10 different questions regarding this image, about object detection ,for other Visual Language Models to answer, \n",
      "            give me only the questions, for example: what objects are there in the image\n"
     ]
    }
   ],
   "source": [
    "# Access the text prompt from the 'content' list\n",
    "text_prompt = None\n",
    "for item in messages[0]['content']:\n",
    "    if item['type'] == 'text':\n",
    "        text_prompt = item['text']\n",
    "        \n",
    "print(text_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbea02c3-2c15-4d77-82a1-a6993cb8c03f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. The woman is sitting on the beach, smiling and playing with a dog.\\n2. The dog is sitting on the beach, with its front paws raised in a high-five gesture.\\n3. The setting is a beach with the ocean in the background.\\n4. The weather appears to be clear and sunny.\\n5. The time of day is likely early morning or late afternoon, as the sun is low in the sky.\\n6. The woman is wearing a plaid shirt and black pants.\\n7. The dog is wearing a harness with a leash attached to it.\\n8. The dog's collar is visible around its neck.\\n9. The dog's breed is not clearly identifiable from the image.\\n10. The dog's name is not visible in the image.\\n11. The woman's name is not visible in the image.\\n12. The woman's profession is not visible in the image.\\n13. The woman's occupation is not visible in the image.\\n14. The woman's hobby is not visible in the image.\\n15. The woman's profession is not visible in the image.\\n16. The woman's occupation is not visible in the image.\\n17. The woman's hobby is not visible in the image.\\n18. The woman's profession is not visible in the image.\\n19. The woman's occupation is not visible in the image.\"]\n"
     ]
    }
   ],
   "source": [
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "output = \" \".join(output_text)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": {output}},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1000)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b531f9d-4a90-44f0-917a-811e7be7e5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
