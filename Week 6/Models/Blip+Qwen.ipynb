{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef88e6-3bb8-41be-a1e6-7b9c9dd4afc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfca85d-768a-4126-969c-df1c448ef2db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "processor_answer = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model_answer = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28f6e6-91b9-40ab-9885-a90412ac3eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input Image ( Loads the image )\n",
    "image_file = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17162f3d-f905-4335-8015-656881bc8a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\" Given the input image, generate 15 unique and diverse questions that can be answered based on the visual content. The questions should cover a wide range of topics such as the following:\n",
    "\n",
    "The main subject or objects in the image.\n",
    "Colors, patterns, and shapes.\n",
    "The actions or movements taking place.\n",
    "The spatial relationships between objects or people.\n",
    "Environmental context (indoor, outdoor, nature, urban, etc.).\n",
    "Emotions, expressions, or moods conveyed.\n",
    "Any interactions between objects, people, or elements.\n",
    "Specific details about the setting or background.\n",
    "Objects or items in the foreground and background.\n",
    "The condition or state of any objects or people (e.g., new, old, active, idle).\n",
    "Make sure to vary the types of questions so they touch on different aspects of the image, and ensure that the questions are easily inferable from the visual content.\n",
    "Ensure each question only has one question to it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c2d7a0bb-a63d-43b4-b9a1-75026325772a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. What is the primary subject of the image?\\n2. What colors are prominent in the image?\\n3. What shapes are present in the image?\\n4. What actions are taking place in the image?\\n5. What is the spatial relationship between the woman and the dog?\\n6. What is the environmental context of the image?\\n7. What emotions or expressions are conveyed in the image?\\n8. What interactions are taking place between the woman and the dog?\\n9. What specific details about the setting or background are visible in the image?\\n10. What objects or items are in the foreground and background of the image?\\n11. What is the condition or state of any objects or people in the image?\\n12. How does the image convey the condition or state of the woman and the dog?\\n13. What is the condition or state of the dog in the image?\\n14. How does the image convey the condition or state of the woman's mood or expression?\\n15. What is the condition or state of the dog's fur or coat in the image?\"]\n"
     ]
    }
   ],
   "source": [
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": f\"{image_file}\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": f\"{prompt}\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1000)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "394ef25a-32f1-421f-8d32-4783fd936c87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the primary subject of the image?', 'What colors are prominent in the image?', 'What shapes are present in the image?', 'What actions are taking place in the image?', 'What is the spatial relationship between the woman and the dog?', 'What is the environmental context of the image?', 'What emotions or expressions are conveyed in the image?', 'What interactions are taking place between the woman and the dog?', 'What specific details about the setting or background are visible in the image?', 'What objects or items are in the foreground and background of the image?', 'What is the condition or state of any objects or people in the image?', 'How does the image convey the condition or state of the woman and the dog?', 'What is the condition or state of the dog in the image?', \"How does the image convey the condition or state of the woman's mood or expression?\", \"What is the condition or state of the dog's fur or coat in the image?\"]\n"
     ]
    }
   ],
   "source": [
    "# Given string (from the list you provided)\n",
    "questions_string = output_text[0]\n",
    "\n",
    "# Replace the escaped \\\\n with actual newline characters\n",
    "questions_string = questions_string.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "# Split the string into individual questions by newline characters\n",
    "questions = questions_string.split(\"\\n\")\n",
    "\n",
    "# Optional: Remove the numbering and \"1.\", \"2.\" part of each question\n",
    "cleaned_questions = [q.split(\". \", 1)[1] for q in questions if q]\n",
    "\n",
    "# Print the result\n",
    "print(cleaned_questions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "117e8f30-6139-4aa3-bd1a-d14a67911aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions have been saved to 'questions.txt'\n"
     ]
    }
   ],
   "source": [
    "# Open a file in write mode\n",
    "with open('questions.txt', 'w') as file:\n",
    "    # Iterate through the list of questions\n",
    "    for question in cleaned_questions:\n",
    "        # Write each question on a new line\n",
    "        file.write(question + '\\n')\n",
    "\n",
    "print(\"Questions have been saved to 'questions.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e63a32fb-48a0-4ffc-88fd-5ff447bc2342",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Given the input image, generate 15 unique and diverse questions that can be answered based on the visual content. The questions should cover a wide range of topics such as the following:\n",
      "\n",
      "The main subject or objects in the image.\n",
      "Colors, patterns, and shapes.\n",
      "The actions or movements taking place.\n",
      "The spatial relationships between objects or people.\n",
      "Environmental context (indoor, outdoor, nature, urban, etc.).\n",
      "Emotions, expressions, or moods conveyed.\n",
      "Any interactions between objects, people, or elements.\n",
      "Specific details about the setting or background.\n",
      "Objects or items in the foreground and background.\n",
      "The condition or state of any objects or people (e.g., new, old, active, idle).\n",
      "Make sure to vary the types of questions so they touch on different aspects of the image, and ensure that the questions are easily inferable from the visual content.\n",
      "Ensure each question only has one question to it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access the text prompt from the 'content' list\n",
    "text_prompt = None\n",
    "for item in messages[0]['content']:\n",
    "    if item['type'] == 'text':\n",
    "        text_prompt = item['text']\n",
    "        \n",
    "print(text_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "741e18ea-2ec1-4cc7-bf83-661f2b906da7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'white and brown', 'dog', 'sitting', 'sitting', 'beach', 'happiness', 'friends', 'beach', 'woman and dog', 'people are in beach', 'happy', 'sad', 'happy', 'wet']\n"
     ]
    }
   ],
   "source": [
    "answers = [] \n",
    "for question in cleaned_questions:\n",
    "    inputs = processor_answer(raw_image, question , return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    out = model_answer.generate(**inputs)\n",
    "    output = processor_answer.decode(out[0], skip_special_tokens=True)\n",
    "    answers.append(output)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f7878b2c-f626-462e-8a58-845a93fe9af1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions and answers have been written to 'questions_answers(Qwen+Blip).json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Pair the questions with answers\n",
    "qa_data = [{\"question\": question, \"answer\": answer} for question, answer in zip(cleaned_questions, answers)]\n",
    "\n",
    "# Write the generated data to a JSON file\n",
    "with open(\"questions_answers(Qwen+Blip).json\", \"w\") as file:\n",
    "    json.dump(qa_data, file, indent=4)\n",
    "\n",
    "print(\"Questions and answers have been written to 'questions_answers(Qwen+Blip).json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "23dcf030-3ccb-40cf-9a75-d07911636d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"question\": \"What is the primary subject of the image?\",\n",
      "        \"answer\": \"dog\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What colors are prominent in the image?\",\n",
      "        \"answer\": \"white and brown\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What shapes are present in the image?\",\n",
      "        \"answer\": \"dog\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What actions are taking place in the image?\",\n",
      "        \"answer\": \"sitting\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What is the spatial relationship between the woman and the dog?\",\n",
      "        \"answer\": \"sitting\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What is the environmental context of the image?\",\n",
      "        \"answer\": \"beach\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What emotions or expressions are conveyed in the image?\",\n",
      "        \"answer\": \"happiness\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What interactions are taking place between the woman and the dog?\",\n",
      "        \"answer\": \"friends\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What specific details about the setting or background are visible in the image?\",\n",
      "        \"answer\": \"beach\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What objects or items are in the foreground and background of the image?\",\n",
      "        \"answer\": \"woman and dog\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What is the condition or state of any objects or people in the image?\",\n",
      "        \"answer\": \"people are in beach\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"How does the image convey the condition or state of the woman and the dog?\",\n",
      "        \"answer\": \"happy\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What is the condition or state of the dog in the image?\",\n",
      "        \"answer\": \"sad\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"How does the image convey the condition or state of the woman's mood or expression?\",\n",
      "        \"answer\": \"happy\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"What is the condition or state of the dog's fur or coat in the image?\",\n",
      "        \"answer\": \"wet\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open('questions_answers(Qwen+Blip).json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the contents of the JSON file\n",
    "print(json.dumps(data, indent=4))  # This will format the JSON for better readability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
