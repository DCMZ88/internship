# Data Curation 

## Aim

- This week, we aim to evaluate a model's performance from their output of a text.
- Before that, Google recently released their new VLM Gemma 3, hence we will test the performance of their model\
  specifically their [Gemma-3-4B-it](https://huggingface.co/google/gemma-3-4b-it)
### Question Generation 
- The questions generated by the Google Gemma3 model are much more verbose and diverse compared to the other models so far.
- Questions generated are not repetitive when prompted to generate more than 20 questions which other models seemed to have struggled so far.
Note: Another consideration could be that this model contains 4B parameters whereas some other models used such as Qwen2 contained 2B parameters.

**Questions generated by Gemma-3**
```
1.  What color is the dog’s fur?
2.  What is the woman wearing?
3.  What is the dog doing?
4.  What type of surface is the woman sitting on?
5.  What is the color of the ocean?
6.  What is visible in the background?
7.  What color is the sky?
8.  What is the woman holding?
9.  What is the dog looking at?
10. What is the woman’s facial expression?
11. Is the scene taking place during the day or night?
12. What kind of weather is it?
13. What is the texture of the sand?
14. What is the dog’s posture?
15. What is the leash pattern like?
16. What is the woman doing with her hand?
17. What is the overall mood of the image?
18. Are there any waves visible?
19. What is the lighting like in the image?
```
# Data Analysis 

Now, we attempt to evaluate the performance of the object detection capabilities of different VLMs.

## Methodology 

### First Attempt 

1. Ask the VLMs to detect and list out the objects detected in the image
2. Pass the list through a object detection model.
3. If the object is detected through the model, record as success
4. Calculate success Rate

Challenges and Limitations:
-  Assume that the Object Detection Model is the ground truth

**First Step**

Prompting the each VLM to detect all the images

<p align="center">
  <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg" width="500" />
  <br>Figure 1: Input Image
</p>

Prompt used
```
Task: 
- Detect all the objects in the image and list them out
- Ensure that the output only includes the labels of the object
```

**Second Step**

Pass each list through the Object Detection Model.

For the object detection model , we used [GroundingDINO](#https://github.com/IDEA-Research/GroundingDINO/blob/main/README.md) by IDEA-Research.

**Results**

1. **Google's Gemma-3-3B-it**

**Labels**
```
['Dog', 'Woman', 'Beach', 'Sand', 'Ocean', 'Sky', 'Hand', 'Clothing', 'Leash', 'Wave']
```

```
Label : dog , Successfully identified in the image
Label : woman , Successfully identified in the image
Label : beach , Successfully identified in the image
Label : sand , Successfully identified in the image
Label : ocean , Successfully identified in the image
Label : sky , Successfully identified in the image
Label : hand , Successfully identified in the image
Label : clothing , Successfully identified in the image
Label : leash , Incorrectly Identified in the image
Label : wave , Incorrectly Identified in the image
Model has a success rate of 80.0% .
```

Sucess Rate : 80%



### Second Attempt

1. Generate questions regarding object detection using Google's Gemma-3-3b-it.
2. Pass these questions into various VLMs to obtain their respective answers
3. Pass these answers to an object detection model to see if it is able to detect the object. ( Check for hallucination )
4. If it is able to detect, record as success . Else , record as failure.
5. Calculate the correct rate.

Limitations:
- We are assuming that the OD model produces the ground truth and does not hallucinate
- Even if the OD model is able to detect the object given by the answer, this does not necessarily mean that the answer to the question is 100% correct.
- If some questions are too specific, ( e.g Shape of wave ), the answer may be correct but the object detector model may not be able to detect it.

**Step One**

We generate 25 questions with the focus on Object Detection using Google's Gemma3.

Sample questions
```
 What is sitting on the sand?
 What is the person wearing?
 What is beside the person’s leg?
 What is in the background?
 What is on the ground near the dog?
 What is the person holding?
 What type of animal is visible?
 What is the surface beneath them?
 What is the dog wearing?
```

I specifically prompted it to generate questions such that the answers will be based on objects in the image. 

<p align="center">
  <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg" width="500" />
  <br>Figure 1: Input Image
</p>

**Step Two**

We then pass these questions into different VLMs for their respective answers.\

For Gemma-3-3B-it:



**Gemma-3-3b-it**

We pass these questions into the Google's Gemma model.

I specified that the prompt such that the output answer to be concise and precise. 
 
