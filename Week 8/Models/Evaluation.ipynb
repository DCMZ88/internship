{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f74010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Owlv2 for the object detection model and to evaluate different VLMs performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures that there is enough memory allocation for the model to load\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "\n",
    "processor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde2f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_labels(labels_list, objects,image_path):\n",
    "    incorrect = [item for item in labels_list if item not in objects]\n",
    "    data = {\n",
    "        \"Image Path\" : image_path,\n",
    "        \"Original Labels\" : labels_list, \n",
    "        \"Correct Labels\" :  objects,\n",
    "        \"Incorrect Labels\" : incorrect\n",
    "    }\n",
    "    \n",
    "    new_path = '/home/jovyan/Evaluation/Labels'\n",
    "    \n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path) # Create the directory if it doesn't exist\n",
    "        print(f\"{new_path} successfully created\")\n",
    "    name = image_path.strip(\".jpg\").strip(\".png\")\n",
    "    json_filename = f'{name}.json'\n",
    "    file_name = os.path.join(new_path , json_filename)\n",
    "    with open(file_name , 'w') as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "    print(f\"File : {json_filename} successfully created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffaf3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(file):\n",
    "    # Open and read the JSON file\n",
    "    path = os.path.join('/home/jovyan/Evaluation/Data', file)\n",
    "    with open(path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feed202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate variables \n",
    "success = 0 \n",
    "# Path to the directory (library)\n",
    "directory_path = \"/home/jovyan/Evaluation/Data\"  # Replace this with the actual path\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "# Filter out directories and show only files\n",
    "files = [file for file in files if os.path.isfile(os.path.join(directory_path, file))]\n",
    "\n",
    "# Print the list of files\n",
    "for index, file in enumerate(files):\n",
    "    torch.cuda.empty_cache()\n",
    "    image_path = file.strip(\".json\")\n",
    "    print(f\"Processing Image {image_path}\")\n",
    "    original_image = os.path.join('/home/jovyan/images',image_path)\n",
    "    image = Image.open(original_image)\n",
    "    image = image.convert(\"RGB\")\n",
    "    \n",
    "    data = load_files(file)\n",
    "    \n",
    "    labels_list = data['Labels']\n",
    "    texts = [labels_list]\n",
    "\n",
    "    inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(\"cuda\") for key, value in inputs.items()}  \n",
    "\n",
    "    correct = 0\n",
    "    objects = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      outputs = model(**inputs)\n",
    "\n",
    "    # Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "    target_sizes = torch.Tensor([image.size[::-1]])\n",
    "    # Convert outputs (bounding boxes and class logits) to Pascal VOC Format (xmin, ymin, xmax, ymax)\n",
    "    results = processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=0.2)\n",
    "    i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "    text = texts[i]\n",
    "    boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        if text[label] in texts[0]:\n",
    "            if text[label] not in objects:\n",
    "                correct += 1 \n",
    "                objects.append(text[label])\n",
    "    success += correct/len(labels_list) * 100\n",
    "    \n",
    "    save_labels(labels_list,objects,image_path)\n",
    "    \n",
    "    print(f\"Successfully processed Image {file}, {index+1}/{len(files)} Images Processed\")\n",
    "\n",
    "rate = (success / len(files))\n",
    "print(f\"Model has a success rate of {round(rate,1)}% over {index + 1} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f0d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = (success / len(files))\n",
    "print(f\"Model has a success rate of {round(rate,1)}% over {index + 1} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309bac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
