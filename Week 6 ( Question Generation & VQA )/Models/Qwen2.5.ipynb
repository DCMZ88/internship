{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c513a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Libraries\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af892595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures that there is enough memory allocation for the model to load\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd5fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model_QWEN = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", attn_implementation='eager', device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processor\n",
    "processor_QWEN = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8923d900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input Image\n",
    "url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f71cd172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of questions to be generated \n",
    "questions = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8878ffec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a young woman sitting on a sandy beach with her golden retriever dog. The woman is wearing a plaid shirt and black pants and is holding the dog's leash. The dog is sitting on the sand and is looking up at the woman with a smile on its face. The ocean can be seen in the background with waves crashing onto the shore. The sky is orange and pink, indicating that it is either sunrise or sunset. The overall mood of the image is peaceful and serene.\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your .txt file\n",
    "file_path = 'caption.txt'  # Replace with your file's path\n",
    "\n",
    "try:\n",
    "    # Open the file in read mode\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read the contents of the file\n",
    "        caption = file.read()\n",
    "        \n",
    "        # Print the contents of the file\n",
    "        print(caption)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file at {file_path} was not found.\")\n",
    "except IOError:\n",
    "    print(\"Error: An error occurred while reading the file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d030355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input prompt \n",
    "prompt = f\"\"\"\n",
    "Task: Given the image input and the caption provided, generate {questions} simple, clear, and unique questions about the image. Each question should focus on one specific aspect of the scene and be easy to understand. The questions should be varied in type and explore different general aspects of the image, but each question should only contain one part.\n",
    "\n",
    "Caption:\n",
    "\"{caption}\"\n",
    "\n",
    "Instructions:\n",
    "- Generate {questions} distinct questions, each focusing on one unique detail or aspect of the scene.\n",
    "- Ensure each question is simple and contains only one part (e.g., \"What is the expression on the character's face?\" or \"What is the color of the sky?\").\n",
    "- Questions should explore different general aspects, such as:\n",
    "    - The appearance or actions of any characters (people, animals, etc.)\n",
    "    - The environment (natural elements like the sky, ocean, land, etc.)\n",
    "    - Emotions or mood conveyed by the scene\n",
    "    - Time of day or lighting (e.g., sunrise, sunset, bright, dark, etc.)\n",
    "    - The relationship between characters (if applicable)\n",
    "    - Objects or features in the scene (e.g., clothing, accessories, weather conditions)\n",
    "- Avoid compound questions or combining more than one query in a single question.\n",
    "- Each question should explore a different aspect of the scene in a clear and simple manner.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08ff6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba485886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. What is the color of the sky in the image?\\n2. What is the dog's breed?\\n3. How is the woman dressed?\\n4. What is the dog doing?\\n5. What is the woman holding?\\n6. What time of day does the image appear to be taken?\\n7. What is the ocean doing?\\n8. What is the woman's expression?\\n9. What is the dog's expression?\\n10. What is the color of the sand?\\n11. What is the woman wearing?\\n12. What is the dog wearing?\\n13. What is the woman's hair color?\\n14. What is the dog's fur color?\\n15. What is the woman's posture?\\n16. What is the dog's posture?\\n17. What is the woman's clothing style?\\n18. What is the dog's activity?\\n19. What is the woman's mood?\"]\n"
     ]
    }
   ],
   "source": [
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": f\"{url}\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": f\"{prompt}\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor_QWEN.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor_QWEN(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "with torch.no_grad():\n",
    "    generated_ids = model_QWEN.generate(**inputs, max_new_tokens=1000)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor_QWEN.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "478b65b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the color of the sky in the image?', \"What is the dog's breed?\", 'How is the woman dressed?', 'What is the dog doing?', 'What is the woman holding?', 'What time of day does the image appear to be taken?', 'What is the ocean doing?', \"What is the woman's expression?\", \"What is the dog's expression?\", 'What is the color of the sand?', 'What is the woman wearing?', 'What is the dog wearing?', \"What is the woman's hair color?\", \"What is the dog's fur color?\", \"What is the woman's posture?\", \"What is the dog's posture?\", \"What is the woman's clothing style?\", \"What is the dog's activity?\", \"What is the woman's mood?\"]\n"
     ]
    }
   ],
   "source": [
    "# Given string (from the list you provided)\n",
    "questions_string = output_text[0]\n",
    "\n",
    "# Replace the escaped \\\\n with actual newline characters\n",
    "questions_string = questions_string.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "# Split the string into individual questions by newline characters\n",
    "questions = questions_string.split(\"\\n\")\n",
    "\n",
    "# Optional: Remove the numbering and \"1.\", \"2.\" part of each question\n",
    "cleaned_questions = [q.split(\". \", 1)[1] for q in questions if q]\n",
    "\n",
    "# Print the result\n",
    "print(cleaned_questions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24e09bee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions have been saved to 'questions.txt'\n"
     ]
    }
   ],
   "source": [
    "# Open a file in write mode\n",
    "with open('questions.txt', 'w') as file:\n",
    "    # Iterate through the list of questions\n",
    "    for question in cleaned_questions:\n",
    "        # Write each question on a new line\n",
    "        file.write(question + '\\n')\n",
    "\n",
    "print(\"Questions have been saved to 'questions.txt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
