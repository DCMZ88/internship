{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeef88e6-3bb8-41be-a1e6-7b9c9dd4afc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2d7a0bb-a63d-43b4-b9a1-75026325772a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. What is the woman doing?\\n2. What is the dog doing?\\n3. What is the dog wearing?\\n4. What is the woman wearing?\\n5. What is the weather like?\\n6. What is the time of day?\\n7. What is the dog's breed?\\n8. What is the woman's hairstyle?\\n9. What is the woman's shoe?\\n10. What is the dog's leash?\\n11. What is the dog's collar?\\n12. What is the dog's harness?\\n13. What is the dog's tail?\\n14. What is the dog's front paw?\\n15. What is the dog's back paw?\\n16. What is the dog's body posture?\\n17. What is the dog's facial expression?\\n18. What is the dog's tail color?\\n19. What is the dog's fur color?\\n20. What is the dog's breed?\"]\n"
     ]
    }
   ],
   "source": [
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"\"\"Generate 20 simple questions about the image for image inferencing or object detection that is clearly visible in the image for other visual language models to infer, give me only the questions\"\"\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1000)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e63a32fb-48a0-4ffc-88fd-5ff447bc2342",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 20 simple questions about the image for image inferencing or object detection that is clearly visible in the image for other visual language models to infer, give me only the questions\n"
     ]
    }
   ],
   "source": [
    "# Access the text prompt from the 'content' list\n",
    "text_prompt = None\n",
    "for item in messages[0]['content']:\n",
    "    if item['type'] == 'text':\n",
    "        text_prompt = item['text']\n",
    "        \n",
    "print(text_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbea02c3-2c15-4d77-82a1-a6993cb8c03f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1. The woman is sitting on the beach, smiling and playing with the dog.\\n2. The dog is sitting on the beach, with its front paw raised in a high-five gesture.\\n3. The dog is wearing a harness.\\n4. The woman is wearing a plaid shirt and black pants.\\n5. The weather appears to be sunny and warm, with a clear sky.\\n6. It is likely early morning or late afternoon, as the sun is low in the sky.\\n7. The dog's breed is not clearly identifiable from the image.\\n8. The woman's hairstyle is not visible in the image.\\n9. The woman is not wearing shoes.\\n10. The dog's leash is red.\\n11. The dog's collar is not visible in the image.\\n12. The dog's harness is blue and red.\\n13. The dog's tail is long and bushy.\\n14. The dog's front paw is raised in the air.\\n15. The dog's back paw is not visible in the image.\\n16. The dog's body posture is relaxed and playful.\\n17. The dog's facial expression is happy and playful.\\n18. The dog's tail is a light brown color.\\n19. The dog's fur color is light brown.\\n20. The dog's breed is not clearly identifiable from the image.\"]\n"
     ]
    }
   ],
   "source": [
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "output = \" \".join(output_text)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": {output}},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1000)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
