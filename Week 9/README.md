# Data Analysis

## Evaluation of VLMs on dataset (cont.)

**Aim** : Evaluate the dataset by generating the ground truth from different zero-shot Object Detection models.

### Methodology
1. Using a VLM to generate the labels of the objects detected in the image.
2. To obtain the ground truth , we pass the labels through different models to obtain the "Correct Labels".
3. If the label is correctly identified in all of the models, we classify that label as the ground truth
4. After obtaining the 'Ground Truths' from the various models, we then compare the original label generated by the VLM with the 'Ground Truth'.
5. We then calculate the precision value of the VLM.

However, to calculate the recall value, we have to obtain the False Negatives. This is because our 'Ground Truth' in this case is obtained from the output of the VLM.\
This means that $F_n=0$, as the 'Ground Truth' will be a subset of the labels from the VLM.

Hence, to obtain the False Negatives ( $F_n$ ), I will run the dataset over a standard Object Detection model. 

#### Information
VLM : Google's Gemma3-4b-it\
Zero-Shot Object Detectors : GroundingDINO, Owlv2, OmDet\
Threshold value ( For all models ) : 0.25\
Object Detector : Florence-2


 > I included each images' precision value in a `json` file.

### Challenges and Limitations
- One Limitation is that each 'Ground Truth' for each VLM wil be different due to the fact that the labels generated from the image might slightly differ.
- Some of the object detector models vary alot in determining the VLM's precision when the threshold value is set to 0.25.\
  This causes the 'Ground Truth' produced to be limited due to the labels identified correctly is lesser , as seen from the results, Gemma's precision using Owlv2 is significantly lesser than that of the other models.\
  I decided to find the 'Ground Truth' using 2 methods\
    1 - Using all 3 models\
    2 - Using only GroundingDINO,OmDet\
  and caluclate the precision values respectively.
### Results 

> Dataset consisted of 265 images

**Through each individual object detector model**

Google's Gemma-3-4b-it:
  - Precision ( GroundingDINO ) : 93.7%
  - Precision ( Owlv2 ) : 48.0%
  - Precision ( Omdet ) : 76.1%
> Threshold value (GroundingDINO) : 0.4 , Precision : 77.5%

**Ground Truth**\

Using all 3 zero-shot object detector models

- Precision : 43.3%

Using only Omdet and GroundingDINO

- Precision : 73.9%


# Research & Literature

## Precision - Recall 
Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of the fraction of relevant items among actually returned items while recall is a measure of the fraction of items that were returned among all items that should have been returned. ‘Relevancy’ here refers to items that are postively labeled, i.e., true positives and false negatives.

Precision is calculated by:

$$\frac{T_p}{T_p+F_p}$$

Recall is calculated by:

$$\frac{T_p}{T_p+F_n}$$

where $T_p$ represents True Postives, $F_p$ represents False Postives, $F_n$ represents False Negatives 

One way that made me understand the different terms easier was to simplify them in layman terms.\
i.e 
- True Positives: Correct Labels identified by the model
- False Postitives : Wrong Labels
- False Negatives : Correct Labels that were not identified by the model.
- $T_P+F_N=$ Total number of correct labels
