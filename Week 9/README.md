# Data Analysis

## Evaluation of VLMs on dataset (cont.)

**Aim** : Evaluate the dataset by generating the ground truth from different zero-shot Object Detection models.

### Methodology
1. Using a VLM to generate the labels of the objects detected in the image.
2. To obtain the ground truth , we pass the labels through different Object Detection  models to obtain the "Correct Labels" for each object detection model. 
3. If the label is correctly identified in all of the models, we classify that label as the ground truth
4. After obtaining the 'Ground Truths' from the various models, we then compare the original label generated by the VLM with the 'Ground Truth'.
5. We then calculate the precision value of the VLM.

*Code*

1. [Generating Labels](https://github.com/DCMZ88/internship/tree/main/Week%208/Models/Generating%20Labels)
2. [Correct Labels](https://github.com/DCMZ88/internship/tree/main/Week%209/Evaluation) ( Run all 3 codes, each for different object detector model )
3. [Ground_Truth](https://github.com/DCMZ88/internship/blob/main/Week%209/GroundTruth.ipynb)
4. [Comparison+Calculation](https://github.com/DCMZ88/internship/blob/main/Week%209/Eval(GroundTruth).ipynb) ( Eval(GroundTruth) )

However, to calculate the recall value, we have to obtain the False Negatives. This is because our 'Ground Truth' in this case is obtained from the output of the VLM.\
This means that $F_n=0$, as the 'Ground Truth' will be a subset of the labels from the VLM.

Hence, to obtain the False Negatives ( $F_n$ ), 
1. Compare the 'Ground Truths' amongst all the different VLM models. ( From Week 8 ) 
2. Run the labels through the 'Ground Truths' to filter out the False Negatives for each model.
3. Calculate the recall values for each model respectively.
> Note: Precision values remains the same as it is only dependent on the output of the VLM.

*Code*

[Obtain False Negatives](https://github.com/DCMZ88/internship/blob/main/Week%209/FalseNeg.ipynb)

#### Information
VLM : Google's Gemma3-4b-it\
Zero-Shot Object Detectors : GroundingDINO, Owlv2, OmDet\
Threshold value ( For all models ) : 0.25\
Object Detector : Florence-2-base


 > Note: I included each images' precision value in a `json` file.

### Challenges and Limitations
- One Limitation is that each 'Ground Truth' for each VLM wil be different due to the fact that the labels generated from the image might slightly differ.
- Some of the object detector models vary alot in determining the VLM's precision when the threshold value is set to 0.25.\
  This causes the 'Ground Truth' produced to be limited due to the labels identified correctly is lesser , as seen from the results, Gemma's precision using Owlv2 is significantly lesser than that of the other models.\
  I decided to find the 'Ground Truth' using 2 methods\
    1 - Using all 3 models\
    2 - Using only GroundingDINO,OmDet\
  and caluclate the precision values respectively.
- One limitation faced was that most standard object detector models had very limited classes of labels which resulted in very objects being detected and had to use zero-shot object detector models to aid in generating the 'Ground Truth'
### Results 

> Dataset consisted of 265 images

**Through each individual object detector model**

Google's Gemma-3-4b-it:
  - Precision ( GroundingDINO ) : 93.7
  - Precision ( Owlv2 ) : 48.0
  - Precision ( Omdet ) : 76.1
> Threshold value (GroundingDINO) : 0.4 , Precision : 77.5

**Ground Truth**

Using all 3 zero-shot object detector models

- Precision : 43.3
- Recall : 43.8

Using only Omdet and GroundingDINO

- Precision : 73.9

All the final labels are in the folder labelled 'Labels(GroundTruth)'
# Research & Literature

## Precision - Recall 
Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of the fraction of relevant items among actually returned items while recall is a measure of the fraction of items that were returned among all items that should have been returned. ‘Relevancy’ here refers to items that are postively labeled, i.e., true positives and false negatives.

Precision is calculated by:

$$\frac{T_p}{T_p+F_p}$$

Recall is calculated by:

$$\frac{T_p}{T_p+F_n}$$

where $T_p$ represents True Postives, $F_p$ represents False Postives, $F_n$ represents False Negatives 

One way that made me understand the different terms easier was to simplify them in layman terms.\
i.e 
- True Positives: Correct Labels identified by the model
- False Postitives : Wrong Labels
- False Negatives : Correct Labels that were not identified by the model.
- $T_P+F_N=$ Total number of correct labels
