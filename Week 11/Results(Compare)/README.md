#Results 

**Information**

VLMs : Gemma-3-4b-it, llava-v1.6-mistral-7b-hf, Kimi-VL-A3B-Instruct, Qwen2.5-VL-7B-Instruct

**Methodology**

Compare outputs of model to get 'Ground Truth'


**Results**

Gemma-3-4b-it:

    - Recall : 96.95
    - Precision : 83.82
    
llava-v1.6-mistral-7b-hf:

    - Recall : 76.66
    - Precision : 64.04
 
Kimi-VL-A3B-Instruct:

    - Recall : 96.97
    - Precision : 87.46
    
Qwen2.5-VL-7B-Instruct:

    - Recall: 95.07
    - Precision : 87.46
